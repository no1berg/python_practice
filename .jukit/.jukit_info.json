{"cmd": "### Based on https://pandas.pydata.org/docs/user_guide/10min.html\nimport numpy as np\nimport pandas as pd\n\n\n# Creating a Series by passing a list of values,\n# letting pandas create a default RangeIndex\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\ns\n\n\n#|%%--%%| <MbbgxRzlGk|BzPDAfcPuP>\n\n# Creating a DataFrame by passing a NumPy array with,\n# a datetime index using date_range() and labeled columns::\ndates = pd.date_range(\"20130101\", periods=6)\n\ndates\n#|%%--%%| <BzPDAfcPuP|NhfI8PZZeJ>\n\ndf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\"))\ndf\n\n\n#|%%--%%| <NhfI8PZZeJ|Ywd0MDnBpb>\n\n# Creating a DataFrame by passing a dictionary of objects where,\n# the keys are the column labels and the values are the colum values\ndf2 = pd.DataFrame(\n        {\n            \"A\": 1.0,\n            \"B\": pd.Timestamp(\"20130102\"),\n            \"C\": pd.Series(1, index=list(range(4)), dtype=\"float32\"),\n            \"D\": np.array([3] * 4, dtype=\"int32\"),\n            \"E\": pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n            \"F\": \"foo\",\n            }\n        )\ndf2\n#|%%--%%| <Ywd0MDnBpb|CAKN6giK4K>\n\n# The columns of the resulting DataFrame have different dtypes\ndf2.dtypes\n#|%%--%%| <CAKN6giK4K|OzZVsDcee3>\n\n# Use DataFrame.head() and DataFrame.tail() to view the top and bottom\n# frame respectively\n\ndf.head()\ndf.tail()\n#|%%--%%| <OzZVsDcee3|DHPjG8BGf8>\n\n# Display the DataFrame.index \ndf.index\n#|%%--%%| <DHPjG8BGf8|qRccPMItVS>\n\n# Display the DataFrame.columns\ndf.columns\n\n\n#|%%--%%| <qRccPMItVS|d34wXlbkcq>\n\n# Return a NumPy representation of the underlying data with\n# DataFrame.to_numpy() without the index or column labels\ndf.to_numpy()\n\n#|%%--%%| <d34wXlbkcq|CRI3Sj5MTx>\n\n# Show a quick statistic summary of the data using describe()\ndf.describe()\n\n\n#|%%--%%| <CRI3Sj5MTx|Noeqa0kqOw>\n\n# Transpose the data using DataFrame.T\ndf.T\n\n#|%%--%%| <Noeqa0kqOw|5tmuK80jcj>\n\n# Sort by an axis using DataFrame.sort_index()\ndf.sort_index(axis=1, ascending=False)\n\n#|%%--%%| <5tmuK80jcj|O2HiXplWr8>\n\n# Sort by values using DataFrame.sort_values()\ndf.sort_values(by=\"B\")\n#|%%--%%| <O2HiXplWr8|fwtpvPtOLE>\n\n### Selection\n\n## Getitem([])\n\n# For a DataFrame, passing a single label selects a column and yields a\n# Series equivalent to df.A\ndf[\"A\"]\n\n\n#|%%--%%| <fwtpvPtOLE|RmG4zukLRX>\n\n# For a DataFrame, passing a slice ':' selects matching rows\ndf[0:3]\n\ndf[\"20130101\":\"20130104\"]\n\n#|%%--%%| <RmG4zukLRX|GH09jrraia>\n\n## Selection by label\n\n# Selecting a row matching a label\ndf.loc[dates[0]]\n\n#|%%--%%| <GH09jrraia|CQnieuQe4T>\n\n# Select all rows (':') with a select column labels\ndf.loc[:, [\"A\", \"B\"]]\n\n#|%%--%%| <CQnieuQe4T|OD7tbKDsZi>\n\n# For label slicing, both endpoints are included:\ndf.loc[\"20130102\":\"20130104\", [\"A\", \"B\"]]\n\n#|%%--%%| <OD7tbKDsZi|lO2EJvmApY>\n\n# Selecting a single row and column label returns a scalar:\ndf.loc[dates[0], \"A\"]\n\n#|%%--%%| <lO2EJvmApY|s3BDvJFDoU>\n\n# For getting fast access to a scalar (equivalent to the prior method)\ndf.at[dates[0], \"A\"]\n\n#|%%--%%| <s3BDvJFDoU|siEJlDgdqb>\n\n## Selection by position\n\n# Select via the position of the passed integers:\ndf.iloc[3]\n\n#|%%--%%| <siEJlDgdqb|EMcJiouisW>\n\n# Integer slices act similar to NumPy/Python\ndf.iloc[3:5, 0:2]\n\n#|%%--%%| <EMcJiouisW|Zn7wHdCdt8>\n\n# Lists of integer position locations:\ndf.iloc[[1, 2, 4], [0, 2]]\n\n\n#|%%--%%| <Zn7wHdCdt8|RGpmN5SjmP>\n\n# For slicing rows explicitly\ndf.iloc[1:3, :]\n\n#|%%--%%| <RGpmN5SjmP|uEGqUqHTyT>\n\n# For slicing columns explicitly:\ndf.iloc[:, 1:3]\n\n#|%%--%%| <uEGqUqHTyT|j51QEHZYFG>\n\n# For getting a value explicitly:\ndf.iloc[1, 1]\n\n#|%%--%%| <j51QEHZYFG|NsN0evr5R6>\n\n# For getting fast access to a scalar (equivalent to the prior method)\ndf.iat[1, 1]\n\n#|%%--%%| <NsN0evr5R6|3llKaVg7Mg>\n\n## Boolean indexing\n\n# Select rows where df.A is greater than 0\ndf[df[\"A\"] > 0]\n#|%%--%%| <3llKaVg7Mg|I4LeoIiszY>\n\n# Selecting values from a DataFrame where boolean condition is met\ndf[df > 0]\n\n#|%%--%%| <I4LeoIiszY|IVGJdeKW0q>\n\n# Using isin() method for filtering\ndf2.copy()\n\ndf2[\"E\"] = [\"one\", \"one\", \"two\", \"three\"]\ndf2\n\ndf2[df2[\"E\"].isin([\"two\", \"four\"])]\n\n#|%%--%%| <IVGJdeKW0q|Ntj0g4PPwd>\n\n## Setting\n\n# Setting a new column automatically aligns the data by the indexes\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range(\"20130102\", periods=6))\ns1\n\ndf[\"F\"] = s1\ndf\n\n#|%%--%%| <Ntj0g4PPwd|NcMp1J4YII>\n\n# Setting values by label:\ndf.at[dates[0], \"A\"] = 0\n\n#|%%--%%| <NcMp1J4YII|HNzXgIVf1E>\n\n# Setting values by position:\ndf.iat[0, 1] = 0\n\n#|%%--%%| <HNzXgIVf1E|cOVG1DUzaD>\n\n# The results of the prior setting operations:\ndf\n\n#|%%--%%| <cOVG1DUzaD|6Tsvpqu7Mi>\n\n# A 'Where' operation with setting:\ndf2 = df.copy()\n\ndf2[df2 > 0] = -df2\n\ndf2\n\n#|%%--%%| <6Tsvpqu7Mi|zQ6ThPnLce>\n\n### Missing data\n\n# For NumPy data types, np.nan represents missing data. It is be default\n# not included in computations\n\n# Reindexing allows you to change/add/delete the index on a specific axis\n# This returns a copy of the data:\ndf1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [\"E\"])\ndf1.loc[dates[0] : dates[1], \"E\"] = 1\ndf1\n#|%%--%%| <zQ6ThPnLce|ii9XHub2zK>\n\n# DataFrame.dropna() drops any rows that have missing data:\ndf1.dropna(how=\"any\")\n\n#|%%--%%| <ii9XHub2zK|l1akH1eFgh>\n\n# DataFrame.fillna() fills missing data:\ndf1.fillna(value=5)\n\n#|%%--%%| <l1akH1eFgh|gMJQ3L6lbk>\n\n# isna() gets the boolean mask where values are nan:\npd.isna(df1)\n\n#|%%--%%| <gMJQ3L6lbk|IbXO3phvVw>\n\n### Operations\n\n## Stats\n\n# Operations in general 'exclude' missing data\n\n# Calculate the mean value for each column:\ndf.mean()\n\n#|%%--%%| <IbXO3phvVw|2TWYfB53ju>\n\n# Calculate the mean value for each row:\ndf.mean(axis=1)\n\n#|%%--%%| <2TWYfB53ju|pen3yRt88e>\n\n# Operating with another Series or DataFrame with a different index or\n# column will align the result with the union of the index or columnn labels\n# In addition, pandas automatically broadcasts along the specified\n# dimension and will fill unaligned labels with np.nan\ns = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)\ns\n\ndf.sub(s, axis=\"index\")\n#|%%--%%| <pen3yRt88e|4twCUSf8uO>\n\n## User defined functions\n\n# DataFrame.agg() and DataFrame.transform() applies a user defined function\n# that reduces or broadcasts its return respectively\ndf.agg(lambda x: np.mean(x) * 5.6)\n\ndf.transform(lambda x: x * 101.2)\n\n\n#|%%--%%| <4twCUSf8uO|48O7j3P2Qs>\n\n## Value counts\n\ns = pd.Series(np.random.randint(0, 7, size=10))\ns\n\ns.value_counts()\n\n#|%%--%%| <48O7j3P2Qs|mUqF7ljI35>\n\n## String methods\n\n# Series is equipped with a set of string processing methods in the str\n# attribute that make it easy to operate on each element of the array,\n# as in the code snippet below\ns = pd.Series([\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"])\ns.str.lower()\n\n#|%%--%%| <mUqF7ljI35|Y6775msqoq>\n\n### Merge\n\n## Concat\n\n# Pandas provides various facilities for easily combining together Series,\n# and DataFrame objects with various kinds of logic for the indexes and\n# and relational algebra functionality in the case of join/merge-type operations\n\n# Concatenating pandas objects together row-wise with concat()\ndf = pd.DataFrame(np.random.randn(10, 4))\ndf\n\n# Break it into pieces\npieces = [df[:3], df[3:7], df[7:]]\npd.concat(pieces)\n#|%%--%%| <Y6775msqoq|gpXpYsbG6I>\n\n## Join\n\n# merge() enables SQL style join types along specific columns:\nleft = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"lval\": [1, 2]})\nright = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"rval\": [4, 5]})\n\nprint(left)\nprint(right)\n\npd.merge(left, right, on=\"key\")\n\n#|%%--%%| <gpXpYsbG6I|tz5XJGNx6w>\n\n# merge() on unique keys:\nleft = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"lval\": [1, 2]})\nright = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"rval\": [4, 5]})\n\nprint(left)\nprint(right)\n\npd.merge(left, right, on=\"key\")\n\n#|%%--%%| <tz5XJGNx6w|EoCrA7YMLH>\n\n### Grouping\n\n# By 'group by' we are referring to a process involving one or more\n# of the following steps\n\n#       * Splitting the data into groups based on some criteria\n#       * Applying a function to each group independently\n#       * Combining the results into a data structure\n\ndf = pd.DataFrame(\n    {\n        \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n        \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n        \"C\": np.random.randn(8),\n        \"D\": np.random.randn(8),\n    }\n)\n\ndf\n#|%%--%%| <EoCrA7YMLH|KMXgk2Ltq8>\n\n# Grouping by a column label, selecting column labels, and then applying the\n# DataFrameGroupBy.sum() function to the resulting groups\n\ndf.groupby(\"A\")[[\"C\", \"D\"]].sum()\n\n#|%%--%%| <KMXgk2Ltq8|V0Dt3hdy30>\n\n# Grouping by multiple columns label MultiIndex\ndf.groupby([\"A\", \"B\"]).sum()\n\n#|%%--%%| <V0Dt3hdy30|XRfeN4bE6j>\n\n### Reshaping\n\n## Stack\n\narrays = [\n   [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n   [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n]\n\nindex = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\n\ndf = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"])\n\ndf2 = df[:4]\ndf2\n\n#|%%--%%| <XRfeN4bE6j|1P9JVtmnqo>\n\n# The stack() method \"compresses\" a level in the DataFrame's columns\nstacked = df2.stack(future_stack=True)\nstacked\n\n#|%%--%%| <1P9JVtmnqo|kQYhE5ohXn>\n\n# With a 'stacked' DataFrame or Series (having a MultiIndex as the index),\n# the inverse operation of stack() is unstack(), which by default\n# unstacks the last level:\nstacked.unstack() # Last level\nstacked.unstack(1)\nstacked.unstack(0) \n\n\n#|%%--%%| <kQYhE5ohXn|iB1HJAlnWT>\n\n## Pivot tables\n\ndf = pd.DataFrame(\n    {\n        \"A\": [\"one\", \"one\", \"two\", \"three\"] * 3,\n        \"B\": [\"A\", \"B\", \"C\"] * 4,\n        \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 2,\n        \"D\": np.random.randn(12),\n        \"E\": np.random.randn(12),\n    }\n)\n\ndf\n\n#|%%--%%| <iB1HJAlnWT|pGzOntLHmA>\n\n# pivot_table() pivots a DataFrame specifying the values, index, and columns\npd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=[\"C\"])\n\n#|%%--%%| <pGzOntLHmA|geLqXv87SC>\n\n### Time series\n\n# pandas has simple, powerful, and efficient functionality for performing\n# operations during frequency conversion (e.g., converting secondly data into\n# 5-minutely data). This is extremely common in, but not limited to,\n# financial applications\nrng = pd.date_range(\"1/1/2012\", periods=100, freq=\"s\")\n\nts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\n\nts.resample(\"5Min\").sum()\n#|%%--%%| <geLqXv87SC|aXzhqBocCJ>\n\n# Series.tz_localize() localizes a time series into a time zone\nrng = pd.date_range(\"3/6/2012 00:00\", periods=5, freq=\"D\")\n\nts = pd.Series(np.random.randn(len(rng)), rng)\nts\n\nts_utc = ts.tz_localize(\"UTC\")\nts_utc\n\n#|%%--%%| <aXzhqBocCJ|cOH8sgYEBT>\n\n# Series.tz_convert() converts a time zone aware time series into another\n# time zone\nts_utc.tz_convert(\"US/Eastern\")\n\n#|%%--%%| <cOH8sgYEBT|L5dPlZC7Xw>\n\n# Adding a non-fixed duration (BusinessDay) to a time series\nrng\nrng + pd.offsets.BusinessDay(5)\n\n#|%%--%%| <L5dPlZC7Xw|a2JntdneeI>\n\n### Categoricals\n\n# Pandas can include categorical data in a DataFrame\ndf = pd.DataFrame(\n    {\"id\": [1, 2, 3, 4, 5, 6], \"raw_grade\": [\"a\", \"b\", \"b\", \"a\", \"a\", \"e\"]}\n)\n\n#|%%--%%| <a2JntdneeI|x2oH8qy3Mz>\n\n# Converting the raw grades to a categorical data type\ndf[\"grade\"] = df[\"raw_grade\"].astype(\"category\")\ndf[\"grade\"]\n\n#|%%--%%| <x2oH8qy3Mz|cMKA1l0Fub>\n\n# Rename the categories to more meaningful names\nnew_categories = [\"very good\", \"good\", \"very bad\"]\ndf[\"grade\"] = df[\"grade\"].cat.rename_categories(new_categories)\ndf[\"grade\"]\n#|%%--%%| <cMKA1l0Fub|lP0fXSMRI8>\n\n# Reorder the categories and simultaneously add the missing categories\n# (methods under Series.cat() return a new Series by default)\ndf[\"grade\"] = df[\"grade\"].cat.set_categories(\n        [\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"]\n        )\ndf[\"grade\"]\n\n#|%%--%%| <lP0fXSMRI8|RU6AiKRtsF>\n\n# Sorting is per order in the categories, not lexical order\ndf.sort_values(by=\"grade\")\n\n#|%%--%%| <RU6AiKRtsF|WxGIbn3xbq>\n\n# Grouping by a categorical column with observed=False also\n# shows empty categories\ndf.groupby(\"grade\", observed=False).size()\n\n#|%%--%%| <WxGIbn3xbq|vaoN1WNv52>\n\n### Plotting\n\n# Using the standard convention for referencing the matplotlib API:\nimport matplotlib.pyplot as plt\nplt.close(\"all\")\n\n# the plt.close method is used to close a figure window\n\n#|%%--%%| <vaoN1WNv52|Ti3woCaOdi>\n\nts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nts = ts.cumsum()\n\nts.plot();\n\n\n#|%%--%%| <Ti3woCaOdi|Kb6fIhq0xB>\n\n# plot() plots all columns:\ndf = pd.DataFrame(\n    np.random.randn(1000, 4), index=ts.index, columns=[\"A\", \"B\", \"C\", \"D\"]\n)\n\n\ndf = df.cumsum()\n\nplt.figure();\n\ndf.plot();\n\nplt.legend(loc='best');\n\n#|%%--%%| <Kb6fIhq0xB|sJJVTMk2Es>\n\n### Importing and Exporting data\n\n## CSV\n\n# Writing to a csv file: using DataFrame.to:csv()\ndf = pd.DataFrame(np.random.randint(0, 5, (10, 5)))\ndf.to_csv(\"foo.csv\")\n\n\n#|%%--%%| <sJJVTMk2Es|SC2RQuHuCm>\n\n# Reading from a csv file: using read_csv()\npd.read_csv(\"foo.csv\")\n\n#|%%--%%| <SC2RQuHuCm|JQ1k5axHwe>\n\n## Parquet\n\n# Writing to a Parquet file:\ndf.to_parquet(\"foo.parquet\")\n\n#|%%--%%| <JQ1k5axHwe|tgt6nuRTHe>\n\n# Reading from a Parquet file:\npd.read_parquet(\"foo.parquet\")\n#|%%--%%| <tgt6nuRTHe|CEE8fWJZWq>\n\n## Excel\n\n# Writing to an excel file:\ndf.to_excel(\"foo.xlsx\", sheet_name=\"Sheet1\")\n\n#|%%--%%| <CEE8fWJZWq|6UsNUAZ7lg>\n\n# Reading from an excel file:\npd.read_excel(\"foo.xlsx\", \"Sheet1\", index_col=None, na_values=[\"NA\"])", "cmd_opts": " -s --md_cell_start=r\\\"\\\"\\\"°°°", "import_complete": 1, "terminal": "nvimterm"}